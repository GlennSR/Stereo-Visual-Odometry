{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GlennSR/Stereo_Visual_Odometry/blob/main/my_stereo_visual_odometry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXwrbX70cDNn"
      },
      "source": [
        "# **Questão 2 - Odometria Visual com câmera stereo**\n",
        "O objetivo dessa questão é de desenvolver um algoritmo de estimação do deslocamento de um carro utilizando como dados de entrada imagens de duas câmeras posicionadas no capô e espaçadas entre si de 0.573 m.\n",
        "\n",
        "Para resolver essa questão, eu busquei auxílio em dois suportes que principais, meu material de curso sobre Visão Geométrica (Matéria aprendida durante meu Master na França) e em um projeto de Odometria Visual que encontrei durante minhas buscas sobre Estado da Arte. O link para esse projeto é https://github.com/niconielsen32/ComputerVision/tree/master/VisualOdometry (Vídeo: https://www.youtube.com/watch?v=WV3ZiPqd2G4)\n",
        "\n",
        "Nesse projeto que utilizei como base para desenvolver o meu código, o autor adota uma aproximação um pouco diferente, onde ele considera, além da predição da posição do carro, os valores de Ground Truth, que ele utiliza para calcular os erros da posição estimada em relação à posição real. Mas como no nosso caso nós não temos os valores de Ground Truth, eu exclui essa parte do código e só deixei a parte de estimação.\n",
        "\n",
        "Abaixo deixo o código desenvolvido para responder ao problema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ5I53h3mpTu",
        "outputId": "57942006-d00a-4eeb-8204-0d27d401201c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Stereo_Visual_Odometry' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/GlennSR/Stereo_Visual_Odometry.git\n",
        "%load https://github.com/GlennSR/Stereo_Visual_Odometry/tree/main/KITTI_sequence_3/calib.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-shPKXhcCZI"
      },
      "outputs": [],
      "source": [
        "# Importando as bibliotecas\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.optimize import least_squares\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from Stereo_Visual_Odometry.lib.visualization import plotting\n",
        "# from Stereo_Visual_Odometry.lib.visualization.video import play_trip\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N43qQHCQj0UU"
      },
      "outputs": [],
      "source": [
        "class VisualOdometry():\n",
        "    def __init__(self, data_dir):\n",
        "        self.K_l, self.P_l, self.K_r, self.P_r = self._load_calib(data_dir + '/calib.txt')\n",
        "        self.images_l = self._load_images(data_dir + '/image_l')\n",
        "        self.images_r = self._load_images(data_dir + '/image_r')\n",
        "\n",
        "        block = 11\n",
        "        P1 = block * block * 8\n",
        "        P2 = block * block * 32\n",
        "        self.disparity = cv2.StereoSGBM_create(minDisparity=0, numDisparities=32, blockSize=block, P1=P1, P2=P2)\n",
        "        self.disparities = [\n",
        "            np.divide(self.disparity.compute(self.images_l[0], self.images_r[0]).astype(np.float32), 16)]\n",
        "        self.fastFeatures = cv2.FastFeatureDetector_create()\n",
        "\n",
        "        self.lk_params = dict(winSize=(15, 15),\n",
        "                              flags=cv2.MOTION_AFFINE,\n",
        "                              maxLevel=3,\n",
        "                              criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 50, 0.03))\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_calib(filepath):\n",
        "        \"\"\"\n",
        "        Loads the calibration of the camera\n",
        "        Parameters\n",
        "        ----------\n",
        "        filepath (str): The file path to the camera file\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        K_l (ndarray): Intrinsic parameters for left camera. Shape (3,3)\n",
        "        P_l (ndarray): Projection matrix for left camera. Shape (3,4)\n",
        "        K_r (ndarray): Intrinsic parameters for right camera. Shape (3,3)\n",
        "        P_r (ndarray): Projection matrix for right camera. Shape (3,4)\n",
        "        \"\"\"\n",
        "        with open(filepath, 'r') as f:\n",
        "            params = np.fromstring(f.readline(), dtype=np.float64, sep=' ')\n",
        "            P_l = np.reshape(params, (3, 4))\n",
        "            K_l = P_l[0:3, 0:3]\n",
        "            params = np.fromstring(f.readline(), dtype=np.float64, sep=' ')\n",
        "            P_r = np.reshape(params, (3, 4))\n",
        "            K_r = P_r[0:3, 0:3]\n",
        "        return K_l, P_l, K_r, P_r\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_images(filepath):\n",
        "        \"\"\"\n",
        "        Loads the images\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        filepath (str): The file path to image dir\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        images (list): grayscale images. Shape (n, height, width)\n",
        "        \"\"\"\n",
        "        image_paths = [os.path.join(filepath, file) for file in sorted(os.listdir(filepath))]\n",
        "        images = [cv2.imread(path, cv2.IMREAD_GRAYSCALE) for path in image_paths]\n",
        "        return images\n",
        "\n",
        "    @staticmethod\n",
        "    def _form_transf(R, t):\n",
        "        \"\"\"\n",
        "        Makes a transformation matrix from the given rotation matrix and translation vector\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        R (ndarray): The rotation matrix. Shape (3,3)\n",
        "        t (list): The translation vector. Shape (3)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        T (ndarray): The transformation matrix. Shape (4,4)\n",
        "        \"\"\"\n",
        "        T = np.eye(4, dtype=np.float64)\n",
        "        T[:3, :3] = R\n",
        "        T[:3, 3] = t\n",
        "        return T\n",
        "\n",
        "    def reprojection_residuals(self, dof, q1, q2, Q1, Q2):\n",
        "        \"\"\"\n",
        "        Calculate the residuals\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dof (ndarray): Transformation between the two frames. First 3 elements are the rotation vector and the last 3 is the translation. Shape (6)\n",
        "        q1 (ndarray): Feature points in i-1'th image. Shape (n_points, 2)\n",
        "        q2 (ndarray): Feature points in i'th image. Shape (n_points, 2)\n",
        "        Q1 (ndarray): 3D points seen from the i-1'th image. Shape (n_points, 3)\n",
        "        Q2 (ndarray): 3D points seen from the i'th image. Shape (n_points, 3)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        residuals (ndarray): The residuals. In shape (2 * n_points * 2)\n",
        "        \"\"\"\n",
        "        # Get the rotation vector\n",
        "        r = dof[:3]\n",
        "        # Create the rotation matrix from the rotation vector\n",
        "        R, _ = cv2.Rodrigues(r)\n",
        "        # Get the translation vector\n",
        "        t = dof[3:]\n",
        "        # Create the transformation matrix from the rotation matrix and translation vector\n",
        "        transf = self._form_transf(R, t)\n",
        "\n",
        "        # Create the projection matrix for the i-1'th image and i'th image\n",
        "        f_projection = np.matmul(self.P_l, transf)\n",
        "        b_projection = np.matmul(self.P_l, np.linalg.inv(transf))\n",
        "\n",
        "        # Make the 3D points homogenize\n",
        "        ones = np.ones((q1.shape[0], 1))\n",
        "        Q1 = np.hstack([Q1, ones])\n",
        "        Q2 = np.hstack([Q2, ones])\n",
        "\n",
        "        # Project 3D points from i'th image to i-1'th image\n",
        "        q1_pred = Q2.dot(f_projection.T)\n",
        "        # Un-homogenize\n",
        "        q1_pred = q1_pred[:, :2].T / q1_pred[:, 2]\n",
        "\n",
        "        # Project 3D points from i-1'th image to i'th image\n",
        "        q2_pred = Q1.dot(b_projection.T)\n",
        "        # Un-homogenize\n",
        "        q2_pred = q2_pred[:, :2].T / q2_pred[:, 2]\n",
        "\n",
        "        # Calculate the residuals\n",
        "        residuals = np.vstack([q1_pred - q1.T, q2_pred - q2.T]).flatten()\n",
        "        return residuals\n",
        "\n",
        "    def get_tiled_keypoints(self, img, tile_h, tile_w):\n",
        "        \"\"\"\n",
        "        Splits the image into tiles and detects the 10 best keypoints in each tile\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        img (ndarray): The image to find keypoints in. Shape (height, width)\n",
        "        tile_h (int): The tile height\n",
        "        tile_w (int): The tile width\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        kp_list (ndarray): A 1-D list of all keypoints. Shape (n_keypoints)\n",
        "        \"\"\"\n",
        "        def get_kps(x, y):\n",
        "            # Get the image tile\n",
        "            impatch = img[y:y + tile_h, x:x + tile_w]\n",
        "\n",
        "            # Detect keypoints\n",
        "            keypoints = self.fastFeatures.detect(impatch)\n",
        "\n",
        "            # Correct the coordinate for the point\n",
        "            for pt in keypoints:\n",
        "                pt.pt = (pt.pt[0] + x, pt.pt[1] + y)\n",
        "\n",
        "            # Get the 10 best keypoints\n",
        "            if len(keypoints) > 10:\n",
        "                keypoints = sorted(keypoints, key=lambda x: -x.response)\n",
        "                return keypoints[:10]\n",
        "            return keypoints\n",
        "        # Get the image height and width\n",
        "        h, w, *_ = img.shape\n",
        "\n",
        "        # Get the keypoints for each of the tiles\n",
        "        kp_list = [get_kps(x, y) for y in range(0, h, tile_h) for x in range(0, w, tile_w)]\n",
        "\n",
        "        # Flatten the keypoint list\n",
        "        kp_list_flatten = np.concatenate(kp_list)\n",
        "        return kp_list_flatten\n",
        "\n",
        "    def track_keypoints(self, img1, img2, kp1, max_error=4):\n",
        "        \"\"\"\n",
        "        Tracks the keypoints between frames\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        img1 (ndarray): i-1'th image. Shape (height, width)\n",
        "        img2 (ndarray): i'th image. Shape (height, width)\n",
        "        kp1 (ndarray): Keypoints in the i-1'th image. Shape (n_keypoints)\n",
        "        max_error (float): The maximum acceptable error\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        trackpoints1 (ndarray): The tracked keypoints for the i-1'th image. Shape (n_keypoints_match, 2)\n",
        "        trackpoints2 (ndarray): The tracked keypoints for the i'th image. Shape (n_keypoints_match, 2)\n",
        "        \"\"\"\n",
        "        # Convert the keypoints into a vector of points and expand the dims so we can select the good ones\n",
        "        trackpoints1 = np.expand_dims(cv2.KeyPoint_convert(kp1), axis=1)\n",
        "\n",
        "        # Use optical flow to find tracked counterparts\n",
        "        trackpoints2, st, err = cv2.calcOpticalFlowPyrLK(img1, img2, trackpoints1, None, **self.lk_params)\n",
        "\n",
        "        # Convert the status vector to boolean so we can use it as a mask\n",
        "        trackable = st.astype(bool)\n",
        "\n",
        "        # Create a maks there selects the keypoints there was trackable and under the max error\n",
        "        under_thresh = np.where(err[trackable] < max_error, True, False)\n",
        "\n",
        "        # Use the mask to select the keypoints\n",
        "        trackpoints1 = trackpoints1[trackable][under_thresh]\n",
        "        trackpoints2 = np.around(trackpoints2[trackable][under_thresh])\n",
        "\n",
        "        # Remove the keypoints there is outside the image\n",
        "        h, w = img1.shape\n",
        "        in_bounds = np.where(np.logical_and(trackpoints2[:, 1] < h, trackpoints2[:, 0] < w), True, False)\n",
        "        trackpoints1 = trackpoints1[in_bounds]\n",
        "        trackpoints2 = trackpoints2[in_bounds]\n",
        "\n",
        "        return trackpoints1, trackpoints2\n",
        "\n",
        "    def calculate_right_qs(self, q1, q2, disp1, disp2, min_disp=0.0, max_disp=100.0):\n",
        "        \"\"\"\n",
        "        Calculates the right keypoints (feature points)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        q1 (ndarray): Feature points in i-1'th left image. In shape (n_points, 2)\n",
        "        q2 (ndarray): Feature points in i'th left image. In shape (n_points, 2)\n",
        "        disp1 (ndarray): Disparity i-1'th image per. Shape (height, width)\n",
        "        disp2 (ndarray): Disparity i'th image per. Shape (height, width)\n",
        "        min_disp (float): The minimum disparity\n",
        "        max_disp (float): The maximum disparity\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        q1_l (ndarray): Feature points in i-1'th left image. In shape (n_in_bounds, 2)\n",
        "        q1_r (ndarray): Feature points in i-1'th right image. In shape (n_in_bounds, 2)\n",
        "        q2_l (ndarray): Feature points in i'th left image. In shape (n_in_bounds, 2)\n",
        "        q2_r (ndarray): Feature points in i'th right image. In shape (n_in_bounds, 2)\n",
        "        \"\"\"\n",
        "        def get_idxs(q, disp):\n",
        "            q_idx = q.astype(int)\n",
        "            disp = disp.T[q_idx[:, 0], q_idx[:, 1]]\n",
        "            return disp, np.where(np.logical_and(min_disp < disp, disp < max_disp), True, False)\n",
        "        \n",
        "        # Get the disparity's for the feature points and mask for min_disp & max_disp\n",
        "        disp1, mask1 = get_idxs(q1, disp1)\n",
        "        disp2, mask2 = get_idxs(q2, disp2)\n",
        "        \n",
        "        # Combine the masks \n",
        "        in_bounds = np.logical_and(mask1, mask2)\n",
        "        \n",
        "        # Get the feature points and disparity's there was in bounds\n",
        "        q1_l, q2_l, disp1, disp2 = q1[in_bounds], q2[in_bounds], disp1[in_bounds], disp2[in_bounds]\n",
        "        \n",
        "        # Calculate the right feature points \n",
        "        q1_r, q2_r = np.copy(q1_l), np.copy(q2_l)\n",
        "        q1_r[:, 0] -= disp1\n",
        "        q2_r[:, 0] -= disp2\n",
        "        \n",
        "        return q1_l, q1_r, q2_l, q2_r\n",
        "\n",
        "    def calc_3d(self, q1_l, q1_r, q2_l, q2_r):\n",
        "        \"\"\"\n",
        "        Triangulate points from both images \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        q1_l (ndarray): Feature points in i-1'th left image. In shape (n, 2)\n",
        "        q1_r (ndarray): Feature points in i-1'th right image. In shape (n, 2)\n",
        "        q2_l (ndarray): Feature points in i'th left image. In shape (n, 2)\n",
        "        q2_r (ndarray): Feature points in i'th right image. In shape (n, 2)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Q1 (ndarray): 3D points seen from the i-1'th image. In shape (n, 3)\n",
        "        Q2 (ndarray): 3D points seen from the i'th image. In shape (n, 3)\n",
        "        \"\"\"\n",
        "        # Triangulate points from i-1'th image\n",
        "        Q1 = cv2.triangulatePoints(self.P_l, self.P_r, q1_l.T, q1_r.T)\n",
        "        # Un-homogenize\n",
        "        Q1 = np.transpose(Q1[:3] / Q1[3])\n",
        "\n",
        "        # Triangulate points from i'th image\n",
        "        Q2 = cv2.triangulatePoints(self.P_l, self.P_r, q2_l.T, q2_r.T)\n",
        "        # Un-homogenize\n",
        "        Q2 = np.transpose(Q2[:3] / Q2[3])\n",
        "        return Q1, Q2\n",
        "\n",
        "    def estimate_pose(self, q1, q2, Q1, Q2, max_iter=100):\n",
        "        \"\"\"\n",
        "        Estimates the transformation matrix\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        q1 (ndarray): Feature points in i-1'th image. Shape (n, 2)\n",
        "        q2 (ndarray): Feature points in i'th image. Shape (n, 2)\n",
        "        Q1 (ndarray): 3D points seen from the i-1'th image. Shape (n, 3)\n",
        "        Q2 (ndarray): 3D points seen from the i'th image. Shape (n, 3)\n",
        "        max_iter (int): The maximum number of iterations\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        transformation_matrix (ndarray): The transformation matrix. Shape (4,4)\n",
        "        \"\"\"\n",
        "        early_termination_threshold = 5\n",
        "\n",
        "        # Initialize the min_error and early_termination counter\n",
        "        min_error = float('inf')\n",
        "        early_termination = 0\n",
        "\n",
        "        for _ in range(max_iter):\n",
        "            # Choose 6 random feature points\n",
        "            sample_idx = np.random.choice(range(q1.shape[0]), 6)\n",
        "            sample_q1, sample_q2, sample_Q1, sample_Q2 = q1[sample_idx], q2[sample_idx], Q1[sample_idx], Q2[sample_idx]\n",
        "\n",
        "            # Make the start guess\n",
        "            in_guess = np.zeros(6)\n",
        "            # Perform least squares optimization\n",
        "            opt_res = least_squares(self.reprojection_residuals, in_guess, method='lm', max_nfev=200,\n",
        "                                    args=(sample_q1, sample_q2, sample_Q1, sample_Q2))\n",
        "\n",
        "            # Calculate the error for the optimized transformation\n",
        "            # print('dof: ', opt_res.x)\n",
        "            error = self.reprojection_residuals(opt_res.x, q1, q2, Q1, Q2)\n",
        "            error = error.reshape((Q1.shape[0] * 2, 2))\n",
        "            error = np.sum(np.linalg.norm(error, axis=1))\n",
        "\n",
        "            # Check if the error is less the the current min error. Save the result if it is\n",
        "            if error < min_error:\n",
        "                min_error = error\n",
        "                out_pose = opt_res.x\n",
        "                early_termination = 0\n",
        "            else:\n",
        "                early_termination += 1\n",
        "            if early_termination == early_termination_threshold:\n",
        "                # If we have not fund any better result in early_termination_threshold iterations\n",
        "                break\n",
        "\n",
        "        # Get the rotation vector\n",
        "        r = out_pose[:3]\n",
        "        # Make the rotation matrix\n",
        "        R, _ = cv2.Rodrigues(r)\n",
        "        # Get the translation vector\n",
        "        t = out_pose[3:]\n",
        "        # Make the transformation matrix\n",
        "        transformation_matrix = self._form_transf(R, t)\n",
        "        return transformation_matrix\n",
        "\n",
        "    def get_pose(self, i):\n",
        "        \"\"\"\n",
        "        Calculates the transformation matrix for the i'th frame\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        i (int): Frame index\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        transformation_matrix (ndarray): The transformation matrix. Shape (4,4)\n",
        "        \"\"\"\n",
        "        # Get the i-1'th image and i'th image\n",
        "        img1_l, img2_l = self.images_l[i - 1:i + 1]\n",
        "\n",
        "        # Get teh tiled keypoints\n",
        "        kp1_l = self.get_tiled_keypoints(img1_l, 10, 20)\n",
        "\n",
        "        # Track the keypoints\n",
        "        tp1_l, tp2_l = self.track_keypoints(img1_l, img2_l, kp1_l)\n",
        "\n",
        "        # Calculate the disparitie\n",
        "        self.disparities.append(np.divide(self.disparity.compute(img2_l, self.images_r[i]).astype(np.float32), 16))\n",
        "\n",
        "        # Calculate the right keypoints\n",
        "        tp1_l, tp1_r, tp2_l, tp2_r = self.calculate_right_qs(tp1_l, tp2_l, self.disparities[i - 1], self.disparities[i])\n",
        "\n",
        "        # Calculate the 3D points\n",
        "        Q1, Q2 = self.calc_3d(tp1_l, tp1_r, tp2_l, tp2_r)\n",
        "\n",
        "        # Estimate the transformation matrix\n",
        "        transformation_matrix = self.estimate_pose(tp1_l, tp2_l, Q1, Q2)\n",
        "        return transformation_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_result_path(pred_path, w_title):\n",
        "  pred_path = np.array(pred_path)\n",
        "\n",
        "  pred_x, pred_y = pred_path.T\n",
        "  xs = list(np.array([pred_x]).T)\n",
        "  ys = list(np.array([pred_y]).T)\n",
        "\n",
        "  plt.plot(xs, ys)\n",
        "  plt.xticks(np.arange(-0.5, 1, 0.5))\n",
        "\n",
        "  plt.xlabel('x - m')\n",
        "  plt.ylabel('y - m')\n",
        "  plt.grid()\n",
        "  plt.title(w_title)"
      ],
      "metadata": {
        "id": "ZIRsqEdZiIKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BL6b2zwkCIT"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    data_dir = 'Stereo_Visual_Odometry/KITTI_sequence_3'  # Try KITTI_sequence_2\n",
        "    vo = VisualOdometry(data_dir)\n",
        "\n",
        "    # play_trip(vo.images_l, vo.images_r)  # Comment out to not play the trip\n",
        "\n",
        "    gt_path = []\n",
        "    estimated_path = []\n",
        "    for i in range(6):\n",
        "        if i < 1:\n",
        "            cur_pose = np.eye(4,4)\n",
        "        else:\n",
        "            transf = vo.get_pose(i)\n",
        "            cur_pose = np.matmul(cur_pose, transf)\n",
        "        estimated_path.append((cur_pose[0, 3], cur_pose[2, 3]))\n",
        "    plot_result_path(estimated_path, \"Estimated Path\")\n",
        "    plotting.visualize_paths(estimated_path, \"Stereo Visual Odometry\",\n",
        "                            file_out=os.path.basename(data_dir) + \".html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18ZTPhLxkGAC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "35ff682d-6333-4338-a78d-fd18d4d7ea9c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKH1jTr_p4sq"
      },
      "source": [
        "#####**OBS: Para visualizar o resultado gerado como html, na esquerda do Colab abra a aba \"Arquivos\" e procure por *KITTI_sequence_3.html*, se ele não existir, tente atualizar a pasta clicando no segundo ícone à logo abaixo do nome \"Arquivos\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Referências:**\n",
        "[1]https://www.youtube.com/watch?v=WV3ZiPqd2G4\n",
        "\n",
        "[2]https://github.com/schvarcz/VisualOdometryPython\n",
        "\n",
        "[3]https://github.com/AhmedHisham1/carla-visual-odometry\n"
      ],
      "metadata": {
        "id": "-Pd0jixgvZK5"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "stereo_visual_odometry.ipynb",
      "provenance": [],
      "mount_file_id": "1R7ADCiQS7MDoMXQwGSbZS5Q1cPv68cvW",
      "authorship_tag": "ABX9TyNp6gOjQ4USvfCGq1i57OnJ",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}